{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Controller Training (RL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevpatha\\anaconda3\\envs\\tf\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor using:  cpu\n",
      "Critic using:  cpu\n",
      "episode 0 score -912.3 avg score -912.3 time_steps 200 learning_steps 10\n",
      "episode 1 score -110.1 avg score -511.2 time_steps 400 learning_steps 20\n",
      "episode 2 score -47.0 avg score -356.5 time_steps 600 learning_steps 30\n",
      "episode 3 score -287.4 avg score -339.2 time_steps 800 learning_steps 40\n",
      "episode 4 score -265.8 avg score -324.5 time_steps 1000 learning_steps 50\n",
      "episode 5 score -869.8 avg score -415.4 time_steps 1200 learning_steps 60\n",
      "episode 6 score -53.1 avg score -363.7 time_steps 1400 learning_steps 70\n",
      "episode 7 score -9.1 avg score -319.3 time_steps 1600 learning_steps 80\n",
      "episode 8 score -775.1 avg score -370.0 time_steps 1800 learning_steps 90\n",
      "episode 9 score -717.8 avg score -404.8 time_steps 2000 learning_steps 100\n",
      "episode 10 score -230.1 avg score -388.9 time_steps 2200 learning_steps 110\n",
      "episode 11 score -262.2 avg score -378.3 time_steps 2400 learning_steps 120\n",
      "episode 12 score -180.2 avg score -363.1 time_steps 2600 learning_steps 130\n",
      "episode 13 score -879.7 avg score -400.0 time_steps 2800 learning_steps 140\n",
      "episode 14 score -214.0 avg score -387.6 time_steps 3000 learning_steps 150\n",
      "episode 15 score -188.4 avg score -375.1 time_steps 3200 learning_steps 160\n",
      "episode 16 score -89.6 avg score -358.3 time_steps 3400 learning_steps 170\n",
      "episode 17 score -229.0 avg score -351.2 time_steps 3600 learning_steps 180\n",
      "episode 18 score -375.3 avg score -352.4 time_steps 3800 learning_steps 190\n",
      "episode 19 score -16.6 avg score -335.6 time_steps 4000 learning_steps 200\n",
      "episode 20 score -177.5 avg score -328.1 time_steps 4200 learning_steps 210\n",
      "episode 21 score -82.1 avg score -316.9 time_steps 4400 learning_steps 220\n",
      "episode 22 score -146.1 avg score -309.5 time_steps 4600 learning_steps 230\n",
      "episode 23 score -512.0 avg score -317.9 time_steps 4800 learning_steps 240\n",
      "episode 24 score -41.4 avg score -306.9 time_steps 5000 learning_steps 250\n",
      "episode 25 score -835.6 avg score -327.2 time_steps 5200 learning_steps 260\n",
      "episode 26 score -10.9 avg score -315.5 time_steps 5400 learning_steps 270\n",
      "episode 27 score -448.5 avg score -320.2 time_steps 5600 learning_steps 280\n",
      "episode 28 score -147.8 avg score -314.3 time_steps 5800 learning_steps 290\n",
      "episode 29 score -25.8 avg score -304.7 time_steps 6000 learning_steps 300\n",
      "episode 30 score -57.2 avg score -296.7 time_steps 6200 learning_steps 310\n",
      "episode 31 score -100.3 avg score -290.6 time_steps 6400 learning_steps 320\n",
      "episode 32 score -16.0 avg score -282.2 time_steps 6600 learning_steps 330\n",
      "episode 33 score -12.7 avg score -274.3 time_steps 6800 learning_steps 340\n",
      "episode 34 score -6.7 avg score -266.7 time_steps 7000 learning_steps 350\n",
      "episode 35 score -553.7 avg score -274.6 time_steps 7200 learning_steps 360\n",
      "episode 36 score -138.3 avg score -271.0 time_steps 7400 learning_steps 370\n",
      "episode 37 score -336.8 avg score -272.7 time_steps 7600 learning_steps 380\n",
      "episode 38 score -89.3 avg score -268.0 time_steps 7800 learning_steps 390\n",
      "episode 39 score -49.5 avg score -262.5 time_steps 8000 learning_steps 400\n",
      "episode 40 score -37.1 avg score -257.0 time_steps 8200 learning_steps 410\n",
      "episode 41 score -38.6 avg score -251.8 time_steps 8400 learning_steps 420\n",
      "episode 42 score -143.1 avg score -249.3 time_steps 8600 learning_steps 430\n",
      "episode 43 score -194.6 avg score -248.1 time_steps 8800 learning_steps 440\n",
      "episode 44 score -9.9 avg score -242.8 time_steps 9000 learning_steps 450\n",
      "episode 45 score -72.2 avg score -239.1 time_steps 9200 learning_steps 460\n",
      "episode 46 score -21.0 avg score -234.4 time_steps 9400 learning_steps 470\n",
      "episode 47 score -224.6 avg score -234.2 time_steps 9600 learning_steps 480\n",
      "episode 48 score -29.3 avg score -230.0 time_steps 9800 learning_steps 490\n",
      "episode 49 score -55.5 avg score -226.5 time_steps 10000 learning_steps 500\n",
      "episode 50 score -57.1 avg score -223.2 time_steps 10200 learning_steps 510\n",
      "episode 51 score -15.8 avg score -219.2 time_steps 10400 learning_steps 520\n",
      "episode 52 score -190.3 avg score -218.7 time_steps 10600 learning_steps 530\n",
      "episode 53 score -173.3 avg score -217.8 time_steps 10800 learning_steps 540\n",
      "episode 54 score -42.1 avg score -214.6 time_steps 11000 learning_steps 550\n",
      "episode 55 score -11.6 avg score -211.0 time_steps 11200 learning_steps 560\n",
      "episode 56 score -11.1 avg score -207.5 time_steps 11400 learning_steps 570\n",
      "episode 57 score -11.9 avg score -204.1 time_steps 11600 learning_steps 580\n",
      "episode 58 score -4.5 avg score -200.8 time_steps 11800 learning_steps 590\n",
      "episode 59 score -25.4 avg score -197.8 time_steps 12000 learning_steps 600\n",
      "episode 60 score -21.4 avg score -194.9 time_steps 12200 learning_steps 610\n",
      "episode 61 score -21.9 avg score -192.2 time_steps 12400 learning_steps 620\n",
      "episode 62 score -22.2 avg score -189.5 time_steps 12600 learning_steps 630\n",
      "episode 63 score -152.1 avg score -188.9 time_steps 12800 learning_steps 640\n",
      "episode 64 score -4.9 avg score -186.0 time_steps 13000 learning_steps 650\n",
      "episode 65 score -42.5 avg score -183.9 time_steps 13200 learning_steps 660\n",
      "episode 66 score -13.3 avg score -181.3 time_steps 13400 learning_steps 670\n",
      "episode 67 score -46.2 avg score -179.3 time_steps 13600 learning_steps 680\n",
      "episode 68 score -15.8 avg score -177.0 time_steps 13800 learning_steps 690\n",
      "episode 69 score -84.8 avg score -175.6 time_steps 14000 learning_steps 700\n",
      "episode 70 score -39.5 avg score -173.7 time_steps 14200 learning_steps 710\n",
      "episode 71 score -14.5 avg score -171.5 time_steps 14400 learning_steps 720\n",
      "episode 72 score -38.9 avg score -169.7 time_steps 14600 learning_steps 730\n",
      "episode 73 score -208.5 avg score -170.2 time_steps 14800 learning_steps 740\n",
      "episode 74 score -32.1 avg score -168.4 time_steps 15000 learning_steps 750\n",
      "episode 75 score -11.6 avg score -166.3 time_steps 15200 learning_steps 760\n",
      "episode 76 score -37.0 avg score -164.6 time_steps 15400 learning_steps 770\n",
      "episode 77 score -125.9 avg score -164.1 time_steps 15600 learning_steps 780\n",
      "episode 78 score -32.0 avg score -162.5 time_steps 15800 learning_steps 790\n",
      "episode 79 score -11.2 avg score -160.6 time_steps 16000 learning_steps 800\n",
      "episode 80 score -44.9 avg score -159.2 time_steps 16200 learning_steps 810\n",
      "episode 81 score -13.7 avg score -157.4 time_steps 16400 learning_steps 820\n",
      "episode 82 score -53.6 avg score -156.1 time_steps 16600 learning_steps 830\n",
      "episode 83 score -37.8 avg score -154.7 time_steps 16800 learning_steps 840\n",
      "episode 84 score -8.8 avg score -153.0 time_steps 17000 learning_steps 850\n",
      "episode 85 score -22.5 avg score -151.5 time_steps 17200 learning_steps 860\n",
      "episode 86 score -42.6 avg score -150.2 time_steps 17400 learning_steps 870\n",
      "episode 87 score -27.0 avg score -148.8 time_steps 17600 learning_steps 880\n",
      "episode 88 score -18.4 avg score -147.4 time_steps 17800 learning_steps 890\n",
      "episode 89 score -101.4 avg score -146.9 time_steps 18000 learning_steps 900\n",
      "episode 90 score -99.3 avg score -146.3 time_steps 18200 learning_steps 910\n",
      "episode 91 score -37.1 avg score -145.1 time_steps 18400 learning_steps 920\n",
      "episode 92 score -7.9 avg score -143.7 time_steps 18600 learning_steps 930\n",
      "episode 93 score -11.2 avg score -142.3 time_steps 18800 learning_steps 940\n",
      "episode 94 score -7.4 avg score -140.8 time_steps 19000 learning_steps 950\n",
      "episode 95 score -9.1 avg score -139.5 time_steps 19200 learning_steps 960\n",
      "episode 96 score -30.5 avg score -138.3 time_steps 19400 learning_steps 970\n",
      "episode 97 score -123.9 avg score -138.2 time_steps 19600 learning_steps 980\n",
      "episode 98 score -15.3 avg score -137.0 time_steps 19800 learning_steps 990\n",
      "episode 99 score -14.5 avg score -135.7 time_steps 20000 learning_steps 1000\n",
      "episode 100 score -36.2 avg score -127.0 time_steps 20200 learning_steps 1010\n",
      "... saving models ...\n",
      "episode 101 score -6.1 avg score -125.9 time_steps 20400 learning_steps 1020\n",
      "... saving models ...\n",
      "episode 102 score -41.5 avg score -125.9 time_steps 20600 learning_steps 1030\n",
      "... saving models ...\n",
      "episode 103 score -33.7 avg score -123.3 time_steps 20800 learning_steps 1040\n",
      "... saving models ...\n",
      "episode 104 score -12.5 avg score -120.8 time_steps 21000 learning_steps 1050\n",
      "... saving models ...\n",
      "episode 105 score -4.2 avg score -112.1 time_steps 21200 learning_steps 1060\n",
      "... saving models ...\n",
      "episode 106 score -4.1 avg score -111.7 time_steps 21400 learning_steps 1070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 107 score -9.1 avg score -111.7 time_steps 21600 learning_steps 1080\n",
      "... saving models ...\n",
      "episode 108 score -40.9 avg score -104.3 time_steps 21800 learning_steps 1090\n",
      "... saving models ...\n",
      "episode 109 score -6.3 avg score -97.2 time_steps 22000 learning_steps 1100\n",
      "... saving models ...\n",
      "episode 110 score -22.2 avg score -95.1 time_steps 22200 learning_steps 1110\n",
      "... saving models ...\n",
      "episode 111 score -8.3 avg score -92.6 time_steps 22400 learning_steps 1120\n",
      "... saving models ...\n",
      "episode 112 score -27.6 avg score -91.1 time_steps 22600 learning_steps 1130\n",
      "... saving models ...\n",
      "episode 113 score -8.5 avg score -82.3 time_steps 22800 learning_steps 1140\n",
      "... saving models ...\n",
      "episode 114 score -7.4 avg score -80.3 time_steps 23000 learning_steps 1150\n",
      "... saving models ...\n",
      "episode 115 score -65.1 avg score -79.0 time_steps 23200 learning_steps 1160\n",
      "... saving models ...\n",
      "episode 116 score -64.1 avg score -78.8 time_steps 23400 learning_steps 1170\n",
      "... saving models ...\n",
      "episode 117 score -5.5 avg score -76.6 time_steps 23600 learning_steps 1180\n",
      "... saving models ...\n",
      "episode 118 score -11.9 avg score -72.9 time_steps 23800 learning_steps 1190\n",
      "... saving models ...\n",
      "episode 119 score -10.9 avg score -72.9 time_steps 24000 learning_steps 1200\n",
      "... saving models ...\n",
      "episode 120 score -14.7 avg score -71.2 time_steps 24200 learning_steps 1210\n",
      "... saving models ...\n",
      "episode 121 score -41.0 avg score -70.8 time_steps 24400 learning_steps 1220\n",
      "... saving models ...\n",
      "episode 122 score -62.9 avg score -70.0 time_steps 24600 learning_steps 1230\n",
      "... saving models ...\n",
      "episode 123 score -8.7 avg score -65.0 time_steps 24800 learning_steps 1240\n",
      "... saving models ...\n",
      "episode 124 score -6.1 avg score -64.6 time_steps 25000 learning_steps 1250\n",
      "... saving models ...\n",
      "episode 125 score -19.0 avg score -56.4 time_steps 25200 learning_steps 1260\n",
      "... saving models ...\n",
      "episode 126 score -8.9 avg score -56.4 time_steps 25400 learning_steps 1270\n",
      "... saving models ...\n",
      "episode 127 score -7.6 avg score -52.0 time_steps 25600 learning_steps 1280\n",
      "... saving models ...\n",
      "episode 128 score -95.2 avg score -51.5 time_steps 25800 learning_steps 1290\n",
      "... saving models ...\n",
      "episode 129 score -19.0 avg score -51.4 time_steps 26000 learning_steps 1300\n",
      "... saving models ...\n",
      "episode 130 score -8.7 avg score -50.9 time_steps 26200 learning_steps 1310\n",
      "... saving models ...\n",
      "episode 131 score -25.6 avg score -50.2 time_steps 26400 learning_steps 1320\n",
      "episode 132 score -31.5 avg score -50.3 time_steps 26600 learning_steps 1330\n",
      "episode 133 score -7.9 avg score -50.3 time_steps 26800 learning_steps 1340\n",
      "episode 134 score -6.0 avg score -50.3 time_steps 27000 learning_steps 1350\n",
      "... saving models ...\n",
      "episode 135 score -7.2 avg score -44.8 time_steps 27200 learning_steps 1360\n",
      "... saving models ...\n",
      "episode 136 score -59.5 avg score -44.0 time_steps 27400 learning_steps 1370\n",
      "... saving models ...\n",
      "episode 137 score -47.4 avg score -41.1 time_steps 27600 learning_steps 1380\n",
      "... saving models ...\n",
      "episode 138 score -2.9 avg score -40.3 time_steps 27800 learning_steps 1390\n",
      "... saving models ...\n",
      "episode 139 score -45.5 avg score -40.2 time_steps 28000 learning_steps 1400\n",
      "... saving models ...\n",
      "episode 140 score -29.9 avg score -40.2 time_steps 28200 learning_steps 1410\n",
      "... saving models ...\n",
      "episode 141 score -8.2 avg score -39.9 time_steps 28400 learning_steps 1420\n",
      "... saving models ...\n",
      "episode 142 score -49.0 avg score -38.9 time_steps 28600 learning_steps 1430\n",
      "... saving models ...\n",
      "episode 143 score -11.3 avg score -37.1 time_steps 28800 learning_steps 1440\n",
      "... saving models ...\n",
      "episode 144 score -6.0 avg score -37.0 time_steps 29000 learning_steps 1450\n",
      "... saving models ...\n",
      "episode 145 score -71.4 avg score -37.0 time_steps 29200 learning_steps 1460\n",
      "... saving models ...\n",
      "episode 146 score -19.3 avg score -37.0 time_steps 29400 learning_steps 1470\n",
      "... saving models ...\n",
      "episode 147 score -6.5 avg score -34.8 time_steps 29600 learning_steps 1480\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-778e80b53d9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mn_steps\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mN\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m                 \u001b[0mlearn_iters\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\Research\\MFC_wing\\Github_files\\ppo_torch_conv1d.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m                 \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m                 \u001b[0mold_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_prob_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m                 \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from ppo_torch_conv1d import Agent\n",
    "from utils import plot_learning_curve\n",
    "import MFC_aileron_env_discrete_pytorch_variable_target_obs_is_state as mfc_env\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #env = gym.make('CartPole-v1')\n",
    "    env = mfc_env.MFC_aileron_Env()\n",
    "    N = 20\n",
    "    batch_size = 5\n",
    "    n_epochs = 4\n",
    "    alpha = 0.00003\n",
    "    agent = Agent(n_actions=env.action_space.n, batch_size=batch_size, \n",
    "                    alpha=alpha, n_epochs=n_epochs, \n",
    "                    input_dims=env.observation_space.shape, \n",
    "                    fc1_dims=512, fc2_dims=512, chk_dir='tmp/ppo/GITHUB_X')\n",
    "    n_games = 5000\n",
    "\n",
    "    figure_file = 'plots/mfc_aileron_flex.png'\n",
    "\n",
    "    best_score = env.reward_range[0]\n",
    "    score_history = []\n",
    "\n",
    "    learn_iters = 0\n",
    "    avg_score = 0\n",
    "    n_steps = 0\n",
    "    \n",
    "    for i in range(n_games):\n",
    "        goal_pos = np.random.rand()*4-2\n",
    "        observation = env.reset(goal_pos)\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            action, prob, val = agent.choose_action(observation)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            n_steps += 1\n",
    "            score += reward\n",
    "            agent.remember(observation, action, prob, val, reward, done)\n",
    "            if n_steps % N == 0:\n",
    "                agent.learn()\n",
    "                learn_iters += 1\n",
    "            observation = observation_\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "        \n",
    "        if i>100:\n",
    "            if avg_score > best_score:\n",
    "                best_score = avg_score\n",
    "                agent.save_models()\n",
    "\n",
    "        print('episode', i, 'score %.1f' % score, 'avg score %.1f' % avg_score,\n",
    "                'time_steps', n_steps, 'learning_steps', learn_iters)\n",
    "    x = [i+1 for i in range(len(score_history))]\n",
    "    plot_learning_curve(x, score_history, figure_file)\n",
    "    \n",
    "    agent.save_models(end=True)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Testing on simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import numpy as np\n",
    "from ppo_torch_conv1d import Agent\n",
    "from utils import plot_learning_curve\n",
    "import MFC_aileron_env_discrete_pytorch_variable_target_obs_is_state as mfc_env\n",
    "import tensorflow as tf\n",
    "env = mfc_env.MFC_aileron_Env()\n",
    "N = 20\n",
    "batch_size = 5\n",
    "n_epochs = 4\n",
    "alpha = 0.00003\n",
    "steps_per_test_ep = 200\n",
    "goals = [0,0.5,-0.5,0]\n",
    "goal_len = len(goals)\n",
    "\n",
    "agent = Agent(n_actions=env.action_space.n, batch_size=batch_size, \n",
    "                    alpha=alpha, n_epochs=n_epochs, \n",
    "                    input_dims=env.observation_space.shape, \n",
    "                    fc1_dims=512, fc2_dims=512, chk_dir='tmp/ppo/10obs_AUG5')\n",
    "\n",
    "states = np.zeros((steps_per_test_ep*goal_len+1, 4))\n",
    "observation = env.test_reset()\n",
    "states[0,:] = observation[:,-1]\n",
    "agent.load_models(end=True)\n",
    "for ii, g in enumerate(goals):\n",
    "    env.next_goal(g)\n",
    "    for i in range(steps_per_test_ep):\n",
    "        action, prob, val = agent.choose_action(observation, avg=True)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        observation = observation_\n",
    "        states[ii*steps_per_test_ep+i+1,:] = observation[:,-1]\n",
    "            \n",
    "plt.plot(np.arange(steps_per_test_ep*goal_len+1), (states[:,0]*226+497.5)*30/1023)\n",
    "plt.plot(np.arange(steps_per_test_ep*goal_len+1), (states[:,3]*226+497.5)*30/1023, '--r')\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel('Position (mm)')\n",
    "plt.show()\n",
    "plt.plot(np.arange(steps_per_test_ep*goal_len+1), states[:,2]*26.8+50)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
