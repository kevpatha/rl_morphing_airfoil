{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO controller training (MO)   ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevpatha\\anaconda3\\envs\\tf\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor using:  cpu\n",
      "Critic using:  cpu\n",
      "episode 0 score -1118.4 avg score -1118.4 time_steps 200 learning_steps 10\n",
      "episode 1 score -89.9 avg score -604.1 time_steps 400 learning_steps 20\n",
      "episode 2 score -87.9 avg score -432.0 time_steps 600 learning_steps 30\n",
      "episode 3 score -1029.7 avg score -581.5 time_steps 800 learning_steps 40\n",
      "episode 4 score -423.4 avg score -549.8 time_steps 1000 learning_steps 50\n",
      "episode 5 score -524.2 avg score -545.6 time_steps 1200 learning_steps 60\n",
      "episode 6 score -1225.9 avg score -642.8 time_steps 1400 learning_steps 70\n",
      "episode 7 score -212.1 avg score -588.9 time_steps 1600 learning_steps 80\n",
      "episode 8 score -1575.9 avg score -698.6 time_steps 1800 learning_steps 90\n",
      "episode 9 score -135.0 avg score -642.2 time_steps 2000 learning_steps 100\n",
      "episode 10 score -584.9 avg score -637.0 time_steps 2200 learning_steps 110\n",
      "episode 11 score -430.4 avg score -619.8 time_steps 2400 learning_steps 120\n",
      "episode 12 score -561.9 avg score -615.3 time_steps 2600 learning_steps 130\n",
      "episode 13 score -309.0 avg score -593.5 time_steps 2800 learning_steps 140\n",
      "episode 14 score -129.0 avg score -562.5 time_steps 3000 learning_steps 150\n",
      "episode 15 score -595.7 avg score -564.6 time_steps 3200 learning_steps 160\n",
      "episode 16 score -218.3 avg score -544.2 time_steps 3400 learning_steps 170\n",
      "episode 17 score -625.0 avg score -548.7 time_steps 3600 learning_steps 180\n",
      "episode 18 score -32.2 avg score -521.5 time_steps 3800 learning_steps 190\n",
      "episode 19 score -638.1 avg score -527.3 time_steps 4000 learning_steps 200\n",
      "episode 20 score -94.9 avg score -506.8 time_steps 4200 learning_steps 210\n",
      "episode 21 score -67.6 avg score -486.8 time_steps 4400 learning_steps 220\n",
      "episode 22 score -2337.9 avg score -567.3 time_steps 4600 learning_steps 230\n",
      "episode 23 score -137.2 avg score -549.4 time_steps 4800 learning_steps 240\n",
      "episode 24 score -304.1 avg score -539.5 time_steps 5000 learning_steps 250\n",
      "episode 25 score -217.8 avg score -527.2 time_steps 5200 learning_steps 260\n",
      "episode 26 score -6477.3 avg score -747.5 time_steps 5400 learning_steps 270\n",
      "episode 27 score -734.9 avg score -747.1 time_steps 5600 learning_steps 280\n",
      "episode 28 score -582.1 avg score -741.4 time_steps 5800 learning_steps 290\n",
      "episode 29 score -731.7 avg score -741.1 time_steps 6000 learning_steps 300\n",
      "episode 30 score -1278.0 avg score -758.4 time_steps 6200 learning_steps 310\n",
      "episode 31 score -463.7 avg score -749.2 time_steps 6400 learning_steps 320\n",
      "episode 32 score -838.7 avg score -751.9 time_steps 6600 learning_steps 330\n",
      "episode 33 score -484.6 avg score -744.0 time_steps 6800 learning_steps 340\n",
      "episode 34 score -760.9 avg score -744.5 time_steps 7000 learning_steps 350\n",
      "episode 35 score -124.3 avg score -727.3 time_steps 7200 learning_steps 360\n",
      "episode 36 score -2720.0 avg score -781.1 time_steps 7400 learning_steps 370\n",
      "episode 37 score -98.3 avg score -763.2 time_steps 7600 learning_steps 380\n",
      "episode 38 score -624.3 avg score -759.6 time_steps 7800 learning_steps 390\n",
      "episode 39 score -835.2 avg score -761.5 time_steps 8000 learning_steps 400\n",
      "episode 40 score -54.0 avg score -744.3 time_steps 8200 learning_steps 410\n",
      "episode 41 score -286.9 avg score -733.4 time_steps 8400 learning_steps 420\n",
      "episode 42 score -41.1 avg score -717.3 time_steps 8600 learning_steps 430\n",
      "episode 43 score -141.4 avg score -704.2 time_steps 8800 learning_steps 440\n",
      "episode 44 score -1059.5 avg score -712.1 time_steps 9000 learning_steps 450\n",
      "episode 45 score -418.5 avg score -705.7 time_steps 9200 learning_steps 460\n",
      "episode 46 score -35.2 avg score -691.4 time_steps 9400 learning_steps 470\n",
      "episode 47 score -763.5 avg score -692.9 time_steps 9600 learning_steps 480\n",
      "episode 48 score -233.5 avg score -683.5 time_steps 9800 learning_steps 490\n",
      "episode 49 score -63.5 avg score -671.1 time_steps 10000 learning_steps 500\n",
      "episode 50 score -71.3 avg score -659.4 time_steps 10200 learning_steps 510\n",
      "episode 51 score -161.9 avg score -649.8 time_steps 10400 learning_steps 520\n",
      "episode 52 score -60.1 avg score -638.7 time_steps 10600 learning_steps 530\n",
      "episode 53 score -588.1 avg score -637.8 time_steps 10800 learning_steps 540\n",
      "episode 54 score -14.8 avg score -626.4 time_steps 11000 learning_steps 550\n",
      "episode 55 score -162.7 avg score -618.1 time_steps 11200 learning_steps 560\n",
      "episode 56 score -156.2 avg score -610.0 time_steps 11400 learning_steps 570\n",
      "episode 57 score -51.1 avg score -600.4 time_steps 11600 learning_steps 580\n",
      "episode 58 score -1496.2 avg score -615.6 time_steps 11800 learning_steps 590\n",
      "episode 59 score -254.5 avg score -609.6 time_steps 12000 learning_steps 600\n",
      "episode 60 score -319.3 avg score -604.8 time_steps 12200 learning_steps 610\n",
      "episode 61 score -39.8 avg score -595.7 time_steps 12400 learning_steps 620\n",
      "episode 62 score -41.9 avg score -586.9 time_steps 12600 learning_steps 630\n",
      "episode 63 score -752.9 avg score -589.5 time_steps 12800 learning_steps 640\n",
      "episode 64 score -485.2 avg score -587.9 time_steps 13000 learning_steps 650\n",
      "episode 65 score -619.4 avg score -588.4 time_steps 13200 learning_steps 660\n",
      "episode 66 score -17.5 avg score -579.9 time_steps 13400 learning_steps 670\n",
      "episode 67 score -37.3 avg score -571.9 time_steps 13600 learning_steps 680\n",
      "episode 68 score -26.3 avg score -564.0 time_steps 13800 learning_steps 690\n",
      "episode 69 score -245.9 avg score -559.4 time_steps 14000 learning_steps 700\n",
      "episode 70 score -689.9 avg score -561.3 time_steps 14200 learning_steps 710\n",
      "episode 71 score -312.7 avg score -557.8 time_steps 14400 learning_steps 720\n",
      "episode 72 score -366.0 avg score -555.2 time_steps 14600 learning_steps 730\n",
      "episode 73 score -558.6 avg score -555.2 time_steps 14800 learning_steps 740\n",
      "episode 74 score -835.3 avg score -559.0 time_steps 15000 learning_steps 750\n",
      "episode 75 score -107.9 avg score -553.0 time_steps 15200 learning_steps 760\n",
      "episode 76 score -29.2 avg score -546.2 time_steps 15400 learning_steps 770\n",
      "episode 77 score -177.8 avg score -541.5 time_steps 15600 learning_steps 780\n",
      "episode 78 score -95.4 avg score -535.9 time_steps 15800 learning_steps 790\n",
      "episode 79 score -256.4 avg score -532.4 time_steps 16000 learning_steps 800\n",
      "episode 80 score -106.6 avg score -527.1 time_steps 16200 learning_steps 810\n",
      "episode 81 score -178.8 avg score -522.9 time_steps 16400 learning_steps 820\n",
      "episode 82 score -218.0 avg score -519.2 time_steps 16600 learning_steps 830\n",
      "episode 83 score -106.3 avg score -514.3 time_steps 16800 learning_steps 840\n",
      "episode 84 score -91.8 avg score -509.3 time_steps 17000 learning_steps 850\n",
      "episode 85 score -68.0 avg score -504.2 time_steps 17200 learning_steps 860\n",
      "episode 86 score -19.0 avg score -498.6 time_steps 17400 learning_steps 870\n",
      "episode 87 score -70.9 avg score -493.7 time_steps 17600 learning_steps 880\n",
      "episode 88 score -292.5 avg score -491.5 time_steps 17800 learning_steps 890\n",
      "episode 89 score -12.3 avg score -486.1 time_steps 18000 learning_steps 900\n",
      "episode 90 score -31.7 avg score -481.2 time_steps 18200 learning_steps 910\n",
      "episode 91 score -311.3 avg score -479.3 time_steps 18400 learning_steps 920\n",
      "episode 92 score -29.2 avg score -474.5 time_steps 18600 learning_steps 930\n",
      "episode 93 score -30.5 avg score -469.7 time_steps 18800 learning_steps 940\n",
      "episode 94 score -51.8 avg score -465.3 time_steps 19000 learning_steps 950\n",
      "episode 95 score -124.8 avg score -461.8 time_steps 19200 learning_steps 960\n",
      "episode 96 score -27.4 avg score -457.3 time_steps 19400 learning_steps 970\n",
      "episode 97 score -127.3 avg score -454.0 time_steps 19600 learning_steps 980\n",
      "episode 98 score -51.5 avg score -449.9 time_steps 19800 learning_steps 990\n",
      "episode 99 score -385.4 avg score -449.2 time_steps 20000 learning_steps 1000\n",
      "episode 100 score -147.9 avg score -439.5 time_steps 20200 learning_steps 1010\n",
      "... saving models ...\n",
      "episode 101 score -65.3 avg score -439.3 time_steps 20400 learning_steps 1020\n",
      "episode 102 score -254.2 avg score -441.0 time_steps 20600 learning_steps 1030\n",
      "... saving models ...\n",
      "episode 103 score -7.9 avg score -430.7 time_steps 20800 learning_steps 1040\n",
      "... saving models ...\n",
      "episode 104 score -39.3 avg score -426.9 time_steps 21000 learning_steps 1050\n",
      "... saving models ...\n",
      "episode 105 score -0.1 avg score -421.7 time_steps 21200 learning_steps 1060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... saving models ...\n",
      "episode 106 score -65.2 avg score -410.0 time_steps 21400 learning_steps 1070\n",
      "... saving models ...\n",
      "episode 107 score -13.2 avg score -408.1 time_steps 21600 learning_steps 1080\n",
      "... saving models ...\n",
      "episode 108 score -10.6 avg score -392.4 time_steps 21800 learning_steps 1090\n",
      "... saving models ...\n",
      "episode 109 score -53.9 avg score -391.6 time_steps 22000 learning_steps 1100\n",
      "... saving models ...\n",
      "episode 110 score -15.7 avg score -385.9 time_steps 22200 learning_steps 1110\n",
      "... saving models ...\n",
      "episode 111 score -86.6 avg score -382.5 time_steps 22400 learning_steps 1120\n",
      "... saving models ...\n",
      "episode 112 score -251.9 avg score -379.4 time_steps 22600 learning_steps 1130\n",
      "... saving models ...\n",
      "episode 113 score -53.7 avg score -376.8 time_steps 22800 learning_steps 1140\n",
      "... saving models ...\n",
      "episode 114 score -36.7 avg score -375.9 time_steps 23000 learning_steps 1150\n",
      "... saving models ...\n",
      "episode 115 score -68.6 avg score -370.6 time_steps 23200 learning_steps 1160\n",
      "... saving models ...\n",
      "episode 116 score -49.4 avg score -368.9 time_steps 23400 learning_steps 1170\n",
      "... saving models ...\n",
      "episode 117 score -66.1 avg score -363.3 time_steps 23600 learning_steps 1180\n",
      "episode 118 score -59.6 avg score -363.6 time_steps 23800 learning_steps 1190\n",
      "... saving models ...\n",
      "episode 119 score -16.6 avg score -357.4 time_steps 24000 learning_steps 1200\n",
      "... saving models ...\n",
      "episode 120 score -34.1 avg score -356.8 time_steps 24200 learning_steps 1210\n",
      "episode 121 score -82.1 avg score -356.9 time_steps 24400 learning_steps 1220\n",
      "... saving models ...\n",
      "episode 122 score -104.8 avg score -334.6 time_steps 24600 learning_steps 1230\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8d19c816518a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mn_steps\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mN\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m                 \u001b[0mlearn_iters\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\Research\\MFC_wing\\Github_files\\ppo_torch_conv1d.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    234\u001b[0m                 \u001b[0mtotal_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'betas'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m             F.adam(params_with_grad,\n\u001b[0m\u001b[0;32m    109\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from ppo_torch_conv1d import Agent\n",
    "from utils import plot_learning_curve\n",
    "import MFC_aileron_env_discrete_pytorch_variable_target_obs_is_state_no_overshoot_reward as mfc_env\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #env = gym.make('CartPole-v1')\n",
    "    env = mfc_env.MFC_aileron_Env()\n",
    "    N = 20\n",
    "    batch_size = 5\n",
    "    n_epochs = 4\n",
    "    alpha = 0.00003\n",
    "    agent = Agent(n_actions=env.action_space.n, batch_size=batch_size, \n",
    "                    alpha=alpha, n_epochs=n_epochs, \n",
    "                    input_dims=env.observation_space.shape, \n",
    "                    fc1_dims=512, fc2_dims=512, chk_dir='tmp/ppo/GITHUB_no_overshoot_X')\n",
    "    n_games = 5000\n",
    "\n",
    "    figure_file = 'plots/mfc_aileron_flex.png'\n",
    "\n",
    "    best_score = env.reward_range[0]\n",
    "    score_history = []\n",
    "\n",
    "    learn_iters = 0\n",
    "    avg_score = 0\n",
    "    n_steps = 0\n",
    "    \n",
    "    for i in range(n_games):\n",
    "        goal_pos = np.random.rand()*4-2\n",
    "        observation = env.reset(goal_pos)\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            action, prob, val = agent.choose_action(observation)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            n_steps += 1\n",
    "            score += reward\n",
    "            agent.remember(observation, action, prob, val, reward, done)\n",
    "            if n_steps % N == 0:\n",
    "                agent.learn()\n",
    "                learn_iters += 1\n",
    "            observation = observation_\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "        \n",
    "        if i>100:\n",
    "            if avg_score > best_score:\n",
    "                best_score = avg_score\n",
    "                agent.save_models()\n",
    "\n",
    "        print('episode', i, 'score %.1f' % score, 'avg score %.1f' % avg_score,\n",
    "                'time_steps', n_steps, 'learning_steps', learn_iters)\n",
    "    x = [i+1 for i in range(len(score_history))]\n",
    "    plot_learning_curve(x, score_history, figure_file)\n",
    "    \n",
    "    agent.save_models(end=True)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Test on simulation ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import numpy as np\n",
    "from ppo_torch_conv1d import Agent\n",
    "from utils import plot_learning_curve\n",
    "import MFC_aileron_env_discrete_pytorch_variable_target_obs_is_state_no_overshoot_reward as mfc_env\n",
    "import tensorflow as tf\n",
    "env = mfc_env.MFC_aileron_Env()\n",
    "N = 20\n",
    "batch_size = 5\n",
    "n_epochs = 4\n",
    "alpha = 0.00003\n",
    "steps_per_test_ep = 200\n",
    "goals = [0,0.5,-0.5,0]\n",
    "goal_len = len(goals)\n",
    "\n",
    "agent = Agent(n_actions=env.action_space.n, batch_size=batch_size, \n",
    "                    alpha=alpha, n_epochs=n_epochs, \n",
    "                    input_dims=env.observation_space.shape, \n",
    "                    fc1_dims=512, fc2_dims=512, chk_dir='tmp/ppo/10obs_no_overshoot_SEP6')\n",
    "\n",
    "states = np.zeros((steps_per_test_ep*goal_len+1, 4))\n",
    "observation = env.test_reset()\n",
    "states[0,:] = observation[:,-1]\n",
    "agent.load_models(end=True)\n",
    "for ii, g in enumerate(goals):\n",
    "    env.next_goal(g)\n",
    "    for i in range(steps_per_test_ep):\n",
    "        action, prob, val = agent.choose_action(observation, avg=True)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        observation = observation_\n",
    "        states[ii*steps_per_test_ep+i+1,:] = observation[:,-1]\n",
    "    \n",
    "            \n",
    "plt.plot(np.arange(steps_per_test_ep*goal_len+1), (states[:,0]*226+497.5)*30/1023)\n",
    "plt.plot(np.arange(steps_per_test_ep*goal_len+1), (states[:,3]*226+497.5)*30/1023, '--r')\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel('Position (mm)')\n",
    "plt.show()\n",
    "plt.plot(np.arange(steps_per_test_ep*goal_len+1), states[:,2]*26.8+50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
